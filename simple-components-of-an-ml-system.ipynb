{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Components of an ML System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**: Nothing in this notebook respresents the setup for an _actual_ production system, but is meant instead to be an educational tool representing some of the lightweight capabilities of some of these tools.\n",
    "\n",
    "That being said, each code snippet below is meant to demonstrate some of the things you might see in an ML system that implements MLOps - using _only_ Python! Cool right?\n",
    "\n",
    "But first - some data! Below is some synthetic user data with two input features and a target. Not really indicative of a real scenario, but good for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>target</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.493749</td>\n",
       "      <td>-0.157663</td>\n",
       "      <td>1</td>\n",
       "      <td>a27b0912-0cf8-11ed-899e-b29c4abd48f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.123346</td>\n",
       "      <td>0.365275</td>\n",
       "      <td>1</td>\n",
       "      <td>a27b09ee-0cf8-11ed-899e-b29c4abd48f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.095964</td>\n",
       "      <td>-1.212640</td>\n",
       "      <td>0</td>\n",
       "      <td>a27b0a2a-0cf8-11ed-899e-b29c4abd48f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038876</td>\n",
       "      <td>-0.154388</td>\n",
       "      <td>0</td>\n",
       "      <td>a27b0a52-0cf8-11ed-899e-b29c4abd48f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.364063</td>\n",
       "      <td>-0.956556</td>\n",
       "      <td>0</td>\n",
       "      <td>a27b0a7a-0cf8-11ed-899e-b29c4abd48f4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  target                               user_id\n",
       "0   0.493749  -0.157663       1  a27b0912-0cf8-11ed-899e-b29c4abd48f4\n",
       "1  -0.123346   0.365275       1  a27b09ee-0cf8-11ed-899e-b29c4abd48f4\n",
       "2   1.095964  -1.212640       0  a27b0a2a-0cf8-11ed-899e-b29c4abd48f4\n",
       "3   0.038876  -0.154388       0  a27b0a52-0cf8-11ed-899e-b29c4abd48f4\n",
       "4   1.364063  -0.956556       0  a27b0a7a-0cf8-11ed-899e-b29c4abd48f4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "user_data = pd.read_csv(\"data/user_data.csv\", index_col=0)\n",
    "user_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use this data to create features, train a model, record our experiments, \"deploy\" the latest model and monitor its performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store\n",
    "Feature stores (and feature systems) intend to make it easy to manage, productize, and define features for machine learning systems. They can typically be used for both model training and for low-latency serving. Some technologies to take a look at: Tecton, Feast.\n",
    "\n",
    "The solution below allows us to convert our user data into a dictionary where `user_id` is the key - so we can retrieve features for users we want to make predictions for quickly on the fly. It also doesn't prevent us from reading all of the data in during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Callable, Dict\n",
    "\n",
    "class SuperSimpleFeatureStore:\n",
    "    \n",
    "    def __init__(self, dataframe: pd.DataFrame):\n",
    "        self.dataframe = dataframe\n",
    "        self.feature_dict = dataframe.set_index(\"user_id\").T.to_dict()\n",
    "            \n",
    "    def register_feature(self, name: str, feature_definition: Callable) -> None:\n",
    "        for key in self.feature_dict:\n",
    "            self.feature_dict[key][name] = feature_definition(self.feature_dict[key]) \n",
    "            \n",
    "    def get_user_feature(self, user_id: str) -> Dict:\n",
    "        return self.feature_dict.get(user_id)\n",
    "    \n",
    "    def get_all_data(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(self.feature_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize this class with our data, and then define new features as functions! The function will automatically be applied to our data and create the new features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_1': -0.1233462294856719,\n",
       " 'feature_2': 0.365275089687713,\n",
       " 'target': 1.0,\n",
       " 'feature_3': 0.015214292328332034}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_store = SuperSimpleFeatureStore(user_data)\n",
    "\n",
    "def new_feature(feature_dict: Dict) -> Dict:\n",
    "    return feature_dict[\"feature_1\"] ** 2\n",
    "\n",
    "feature_store.register_feature(\"feature_3\", new_feature)\n",
    "feature_store.get_user_feature(\"a27b09ee-0cf8-11ed-899e-b29c4abd48f4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Metadata Store (Experiment tracking) and Model Registry\n",
    "ML metadata (experiment tracking) is essentially your lab notebook for data science projects. The idea is you capture all metadata and information about experiment runs to make things reproducible. On top of that is a model registry, which would be a more central place to manage and version models. Some tools to look at: MLFlow, Weights and Biases, Comet, Sagemaker.\n",
    "\n",
    "In this case - our design is simpler. We'll just capture all the information about our experiments in a CSV file that we use to track results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Makes sure joblib and scikitlearn are isntalled\n",
    "!{sys.executable} -m pip install joblib\n",
    "!{sys.executable} -m  pip install scikit-learn\n",
    "\n",
    "# Allows python scripts to be run in the background for later\n",
    "get_ipython().system = os.system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import joblib\n",
    "import sklearn.pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "## helper function to save a model/pipeline\n",
    "def save_pipeline(pipeline: sklearn.pipeline.Pipeline, ts: datetime = None) -> str:\n",
    "    model_path = f\"data/pipeline-{ts}.pkl\".replace(\" \",\"-\")\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    return model_path\n",
    "\n",
    "## helper function to write a new line to the csv\n",
    "def record_model(pipeline: sklearn.pipeline.Pipeline, score: float, parameters: dict) -> None:\n",
    "    model = str(pipeline)\n",
    "    ts = datetime.now()\n",
    "    model_path = save_pipeline(pipeline, ts)\n",
    "    f = open('metadata_store.csv', 'a')\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow([ts, model, parameters, score, model_path])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our helper functions, we can do different training runs and records the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model</th>\n",
       "      <th>parameters</th>\n",
       "      <th>score</th>\n",
       "      <th>model_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-26 19:51:56.176934</td>\n",
       "      <td>Pipeline(steps=[('standardscaler', StandardSca...</td>\n",
       "      <td>{'penalty': 'l2', 'C': 1.0, 'max_iter': 100}</td>\n",
       "      <td>0.88</td>\n",
       "      <td>data/pipeline-2022-07-26-19:51:56.176934.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp  \\\n",
       "0  2022-07-26 19:51:56.176934   \n",
       "\n",
       "                                               model  \\\n",
       "0  Pipeline(steps=[('standardscaler', StandardSca...   \n",
       "\n",
       "                                     parameters  score  \\\n",
       "0  {'penalty': 'l2', 'C': 1.0, 'max_iter': 100}   0.88   \n",
       "\n",
       "                                     model_path  \n",
       "0  data/pipeline-2022-07-26-19:51:56.176934.pkl  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## logistic regression parameter config\n",
    "parameters = {\n",
    "    \"penalty\":\"l2\",\n",
    "    \"C\":1.0,\n",
    "    \"max_iter\": 100\n",
    "}\n",
    "\n",
    "## use a standard scaler and logistic regression\n",
    "scaler = StandardScaler()\n",
    "logistic_regression = LogisticRegression(\n",
    "    penalty=parameters[\"penalty\"],\n",
    "    C=parameters[\"C\"],\n",
    "    max_iter=parameters[\"max_iter\"],\n",
    ")\n",
    "\n",
    "## make a pipeline out of them\n",
    "pipeline = make_pipeline(scaler, logistic_regression)\n",
    "\n",
    "## get our data from the feature store and create a train/test split\n",
    "data = feature_store.get_all_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[[\"feature_1\",\"feature_2\",\"feature_3\"]], data[\"target\"])\n",
    "\n",
    "## fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "## get the test score \n",
    "score = pipeline.score(X_test, y_test)\n",
    "\n",
    "## record it\n",
    "record_model(pipeline, score, parameters)\n",
    "\n",
    "## view the output\n",
    "pd.read_csv(\"metadata_store.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Training Pipeline\n",
    "Once model code, data and parameters are optimized, code can be abstracted to a source-controlled repo (git) and the training code can be scheduled and automated. In cases like e-commerce, there is already new data coming in and models frequently need to be retrained. You want to automate the process of training and deploying models whose parameters have been fairly set. Some tools to look at: Airflow, Kubeflow.\n",
    "\n",
    "In perhaps the biggest oversimplification thus far, I've added an infinite loop to the code above and put it in its own Python script. That way, we can run the script in the background to continuously train new models (with a 60 second sleep in between runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 main.py &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Integration\n",
    "Continuous integration is the act of actively committing changes to a central repository, and also covers automated tests and builds. Most of these actions and triggered by git commits and pushes to remote repositories like Github. In this case, I've added 1 test that can be run with `pytest`. It is not automated in this case. Tools to look at: Jenkins, Buildkite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.5, pytest-7.1.2, pluggy-1.0.0\n",
      "rootdir: /workspace\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_example.py \u001b[32m.\u001b[0m\u001b[32m                                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACTION**: practice CI on your own by adding the `pytest` [command as a git pre-commit hook](https://pre-commit.com/) (ie will run whenever you try to commit code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Delivery/Deployment & Model Server\n",
    "Continuous delivery is the practice of reliabily releasing small iterative changes to software to ensure it can be reliably shipped. Continuous deployment is just consistently deploying. In the case of ML this would be part of the automated process - where a model training pipeline automatically ships a newly trained model to a model server. Tools to look at: Jenkins, Buildkite. \n",
    "\n",
    "A model server is typically an HTTP server that accepts prediction input (features) and returns predictions. Tools to look at: Tensorflow Serving, Seldon Core.\n",
    "\n",
    "In this case, instead of doing \"CD\" we're just updating loading the latest trained model every prediction (remember it's updating in the background). Then, we use a `predict` function instead of a model server to fetch features for a given user ID and make a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_latest_model() -> sklearn.pipeline.Pipeline:\n",
    "    latest_model_path = pd.read_csv(\"metadata_store.csv\").query(\"timestamp == timestamp.max()\")[\"model_path\"].values[0]\n",
    "    loaded_model = joblib.load(latest_model_path)\n",
    "    return loaded_model\n",
    "\n",
    "def model_predict(user_id: str) -> float:\n",
    "    model = load_latest_model()\n",
    "    features = feature_store.get_user_feature(user_id)\n",
    "    return model.predict([[v for k,v in features.items() if k != \"target\"]])\n",
    "    \n",
    "model_predict(\"a27b0912-0cf8-11ed-899e-b29c4abd48f4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Monitoring\n",
    "Monitoring and observability of production systems is absolutely one of the most critical components of one. Real systems also have alerting to notify engineers of production issues. In this case, we'll create a simple monitor that records the latency of predictions and reports on the mean. Some tools to look into: Prometheus, Grafana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.inference_times = []\n",
    "    \n",
    "    def record(self, prediction_time: float) -> None:\n",
    "        self.inference_times.append(prediction_time)\n",
    "    \n",
    "    def mean(self) -> float:\n",
    "        return sum(self.inference_times)/len(self.inference_times)\n",
    "        \n",
    "\n",
    "def model_predict(user_id: str) -> float:\n",
    "    start = time.time()\n",
    "    model = load_latest_model()\n",
    "    features = feature_store.get_user_feature(user_id)\n",
    "    prediction = model.predict([[v for k,v in features.items() if k != \"target\"]])\n",
    "    end = time.time()\n",
    "    monitor.record(end-start)\n",
    "    return prediction\n",
    "\n",
    "monitor = PerformanceMonitor()\n",
    "model_predict(\"a27b0912-0cf8-11ed-899e-b29c4abd48f4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033744096755981445"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
